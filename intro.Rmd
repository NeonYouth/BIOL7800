---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
urlcolor: blue
---

# Introduction

|     Owing to the mysticism that often surrounds the subject, "Deep Learning" is a relatively misunderstood technology that has been used for quite innovative applications in Biology within the past half-decade. From identifying cancerous cells from biopsy images that previously went undetected (Shen et al., 2019), to accurately predicting the multi-structural composition of a protein in 3D-space from just its amino-acid sequence (Jumper et al., 2021), these powerful tools in Biology (which span research and industry) owe their existence to a class of algorithms that, at their core, simply excel at recognizing "complex motifs from large sequence data sets" (Rusk, 2016). 
|     One class of those "large sequence data sets" are images. As most people are well-aware, one of, if not the most important senses animals possess is that of sight, as over half of the mammalian brain is dedicated to dealing with the plethora of visual imagery we absorb throughout our lifetimes. This task is made difficult not so much by the process of achieving visual acuity, but interpreting the "meaning" represented by a given image. As in, what objects the image contains, how large or small are those objects are, what their relationship to each other is in 3D-space, etc. 
|     Within ecology and evolution, specifically taxonomy, several computer vision applications have recently been built and made available for public use. Ben Weinstein's DeepMeerkat is one such example, training a model to identify specific species of animal within a set of motion-video, which allows for field-researchers to quickly isolate portions of video containing their species of interest without having to manually search through all of their footage (Weinstein, 2018). iNaturalist, the popular biodiversity social network, has built a large database of specimen images along with temporal and location data, all of which has been used to train several highly-accurate models to identify animals down to their genus and species (Timm et al., 2018) Apart from these relatively basic identification tools, however, computer vision has not often been used to deduce patterns in images beyond those that can be identified by the human eye. 
|     This project, then, is specifically concerned with developing a computer-vision model that can be used to detect complex visual motifs in a given organism that may be later used as previously undetectable traits for phylogenetic analysis. One such class of computer-vision model that I hypothesize will be useful for this task is "instance segmentation," which involves a model that can identify which parts of an image, down to the pixel, comprise an object-of-interest (Ajmal et al., 2018). In our case, that object-of-interest would be a given organism, and I hypothesize that elements of the feature-map generated by an instance segmentation model could be used in place of, or in addition to, simplistic, single-feature taxonomic traits to develop higher-resolution phylogenies. Since developing a brand new method of phylogenetic analysis is, in my estimate, outside the scope of this course, I will use this project to train an instance segmentation model on an arbitrary set of animal images that can be used as a stepping-stone to eventually developing this methodology.
|     The specific model I intend to train is the U-net instance segmentation model. This model was developed at the University of Freiburg in Germany, and was designed to quickly achieve instance segmentation using less than 20 training images. The primary application of this model is instance segmentation of biomedical-specific images, and excels at training on so few because of the higher cost of producing microscopic images (Ronneverger et al., 2015) I am specifically interested in how the model will perform when trained on 3D images of wildlife, as opposed to 2D EM images.  

# Methods

|     The data I used to train the U-Net model is available on [Kaggle](https://www.kaggle.com/metavision/accurate-red-sea-reef-fish-shapessegmentation), a prominent data science community and source of a number of datasets available for public use. This particular dataset contains 1210 images of Red Sea reef fish (Fig 1a), along with 1210 manually-annotated masks: 1 for each image (Fig 1b).

|     In order to satisfy the project requirement of developing within the R language, I used the U-net implementation and guide published by the [RStudio AI Blog](https://blogs.rstudio.com/ai/posts/2019-08-23-unet/). Since it was published in 2019, some modification of this implementation was required, and these changes are documented in the code. This implementation of U-net was built with Tensorflow 2.0 Keras, a popular wrapper library for Tensorflow that allows for quick experimentation through the use of modular "plug-and-play" network layers that can be replaced and modified on the fly. It also uses [Tensorflow for R](https://tensorflow.rstudio.com/installation/) and [r-reticulate](https://rstudio.github.io/reticulate/), both of which act as R wrappers for Python 3. Installation of conda is necessary for both the Tensorflow and Python libraries to be installed in a virtual environment, which can get messy if one loses track of the environment which is automatically created by r-reticulate. For that reason, more specific instructions on how to run this model are included in Apendix A - Installation Instructions. 

|     The actual code involved in training and using the U-Net model is not long at all, about 140 lines in total, so I will include it below in several blocks, and comment on the specific function of each block.

# Training the U-Net model (R Code)

```{r}
model <- unet(input_shape = c(128, 128, 3))
```
The above code creates a U-Net, whose dimensions have been pre-selected by the Keras implementation available [here](https://github.com/r-tensorflow/unet). This specifies the dimensions of the input images, as well as the number of color channels (RGB). The U-Net will expect these of images it trains on, but also those that it recieves as input for mask prediction.

```{r}
data <- tibble(
  img = list.files(here::here("images"), full.names = TRUE),
  mask = list.files(here::here("mask"), full.names = TRUE)
)

data <- initial_split(data, prop = 0.8)
```
I then load the image and mask file names into a tibble, and split the data between a training and testing set. This will set some data aside for training, and set 20% of the images aside for the model to test its accuracy.

```{r}
training_dataset <- training(data) %>%
  tensor_slices_dataset() %>%
  dataset_map(~.x %>% list_modify(
    img = tf$image$decode_jpeg(tf$io$read_file(.x$img)),
    mask = tf$image$decode_jpeg(tf$io$read_file(.x$mask))
  ))

training_dataset <- training_dataset %>%
  dataset_map(~.x %>% list_modify(
    img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),
    mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)
  ))

training_dataset <- training_dataset %>%
  dataset_map(~.x %>% list_modify(
    img = tf$image$resize(.x$img, size = shape(128, 128)),
    mask = tf$image$resize(.x$mask, size = shape(128, 128))
  ))
```
I then perform preprocessing of the images, which involves converting reading them in as jpeg files, converting them to their 32-bit floating-point representations, and downscaling them to 128x128 pixels, the input that the U-Net is expecting.

```{r}
#Data augmentation
random_bsh <- function(img) {
  img %>%
    tf$image$random_brightness(max_delta = 0.3) %>%
    tf$image$random_contrast(lower = 0.5, upper = 0.7) %>%
    tf$image$random_saturation(lower = 0.5, upper = 0.7) %>%
    tf$clip_by_value(0, 1)
}

training_dataset <- training_dataset %>%
  dataset_map(~.x %>% list_modify(
    img = random_bsh(.x$img)
  ))
```
Then data-augmentation is performed on the images. This method will randomly modify non-essential values of the image, like brightness, contrast, and saturation. This is done to control for these variables, such that if an input image varied in any of these dimensions, it should not effect the prediction of the U-Net model.

```{r}
model %>% compile(
  optimizer = optimizer_rmsprop(learning_rate = 1e5),
  loss = "binary_crossentropy",
)

model %>% fit(
  training_dataset,
  epochs = 1,
  validation_data = validation_dataset
)
```
In this final step of training


